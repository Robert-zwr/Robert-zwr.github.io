<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>论文笔记2 - ONE-PEACE</title>
    <link href="/2023/07/16/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B02-ONE-PEACE/"/>
    <url>/2023/07/16/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B02-ONE-PEACE/</url>
    
    <content type="html"><![CDATA[<h1 id="ONE-PEACE-EXPLORING-ONE-GENERAL-REPRESENTATION-MODEL-TOWARD-UNLIMITED-MODALITIES"><a href="#ONE-PEACE-EXPLORING-ONE-GENERAL-REPRESENTATION-MODEL-TOWARD-UNLIMITED-MODALITIES" class="headerlink" title="ONE-PEACE: EXPLORING ONE GENERAL REPRESENTATION MODEL TOWARD UNLIMITED MODALITIES"></a>ONE-PEACE: EXPLORING ONE GENERAL REPRESENTATION MODEL TOWARD UNLIMITED MODALITIES</h1><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>一种多模态预训练方法，可扩展至无限种模态，在不使用任何视觉或语言预训练模型进行初始化的情况下，ONE-PEACE在广泛的单模态和多模态任务中取得了领先的结果。</p><p>ONE-PEACE实际训练了图像、文本、语音三个模态，其中文本为中心模态，以对齐其他两个模态。</p><h2 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h2><img src="https://s2.loli.net/2023/07/16/yjCSwJXHzTYOrDZ.png" style="zoom:67%;" /><ul><li><p>Modality Adapters（对应ViT中的patch_embed层)</p><ul><li><p>Vision Adapter：使用分层MLP（hMLP，<a href="https://zhuanlan.zhihu.com/p/538015554">来源</a>）改进patch_embed层以更好地适应MIM，hMLP等价于三个卷积层（4x4 conv -&gt; 2x2 conv -&gt; 2x2 conv）级联，不同patch之间没有信息交互，和原生ViT的处理（一个stride&#x3D;16且kernel_size&#x3D;16的卷积层）兼容。</p><img src="https://pic1.zhimg.com/80/v2-bf75fdf36acfff930e91372e4d89dec8_1440w.webp" alt="img" style="zoom: 80%;" /></li><li><p>Audio Adapter：</p><ul><li><p>使用wav2vec 2.0中的CNN特征提取器得到audio embeddings。wav2vec 2.0是音频自监督表征学习的经典方法，其使用Z（audio embeddings）”量化“得到的Q，与对Z mask+transformer得到的C进行对比学习，从而达到预训练的目的。</p><p>这里的”量化“不同于模型压缩中的量化，而是”乘积量化“，意思是指把原来的向量空间分解为若干个低维向量空间的笛卡尔积，并对分解得到的低维向量空间分别做量化，从而把连续空间量化成有限空间。具体为把原来连续的特征空间假设是d维，拆分成G个子空间（codebook），每个子空间维度是d&#x2F;G。然后分别在每个子空间里面聚类（K-mean什么的），一共获得V个中心和其中心特征。每个类别的特征用其中心特征代替。</p><p>结果就是，原来d维的连续空间（有无限种特征表达形式），坍缩成了有限离线的空间[GxV]，其可能的特征种类数就只有G*V个。</p><p>乘积量化巧妙在哪儿：</p><p>乘积量化操作通过将无限的特征表达空间坍缩成有限的离散空间，让特征的鲁棒性更强，不会受少量扰动的影响（只要还在某一类里面，特征都由中心特征来代替）。这个聚类过程也是一个特征提取的过程，让特征的表征能力更强了。</p><p><strong>VQVAE中也有类似的量化操作</strong></p><p><a href="https://zhuanlan.zhihu.com/p/531958632">解读</a>、<a href="https://pytorch.org/audio/stable/tutorials/speech_recognition_pipeline_tutorial.html">Pytorch tutorial</a></p><p><img src="https://pic4.zhimg.com/80/v2-825dd85acfbb13e5dee6d817b523a7d3_1440w.webp" alt="img"></p></li><li><p>Audio Adapter没有使用绝对位置嵌入，而是使用卷积层提取相对位置信息并将其添加到音频嵌入中。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">x_conv = self.pos_conv(x.transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>))<br>  x_conv = x_conv.transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>)<br>  x = x + x_conv<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">(encoder): TransformerEncoder(<br>    (pos_conv): Sequential(<br>      (<span class="hljs-number">0</span>): Conv1d(<span class="hljs-number">768</span>, <span class="hljs-number">768</span>, kernel_size=(<span class="hljs-number">128</span>,), stride=(<span class="hljs-number">1</span>,), padding=(<span class="hljs-number">64</span>,), groups=<span class="hljs-number">16</span>)<br>      (<span class="hljs-number">1</span>): SamePad()<br>      (<span class="hljs-number">2</span>): GELU(approximate=none)<br>    )<br></code></pre></td></tr></table></figure></li></ul></li><li><p>Language Adapter：常规，BPE -&gt; add CLS&#x2F;EOS token -&gt; embedding layer</p></li></ul></li><li><p>Modality Fusion Encoder：每个Transformer层中包含一个共享自注意力层和三个模态前馈网络(ffn)，改进点：</p><ul><li><p>Sub-LayerNorm</p><img src="https://pic2.zhimg.com/80/v2-9a0bc3f22d693ec6235336da81dcc929_1440w.webp" alt="img" style="zoom:50%;" /></li><li><p>GeGLU Activation Function：FFN的中间维数设置为嵌入维数的4倍，这与PaLM的做法一致</p></li><li><p>Relative Position Bias (RPB，相对位置偏差)：添加到自注意力层中的<a href="mailto:&#x71;&#x40;&#x6b;&#x2e;&#x54;">&#x71;&#x40;&#x6b;&#x2e;&#x54;</a>中，文本和音频引入1D RPB，图像引入2D RPB。在预训练阶段，不同自注意层的相对位置偏差是共享的。在微调阶段，解耦每个自注意层的相对位置偏差，并让它们继承预训练的相对偏差的权重。</p><p><strong>EVA02代码中有RPB的代码实现</strong></p><p><a href="https://www.zhihu.com/tardis/zm/art/577855860?source_id=1005">RPB解读</a></p></li><li><p>LayerScale：在加入残差之前，我们将每一层(attn层和FFN层)的输出乘以一个可学习的对角矩阵。</p></li></ul></li></ul><h2 id="预训练任务"><a href="#预训练任务" class="headerlink" title="预训练任务"></a>预训练任务</h2><ul><li><p>跨模态对比学习：与CLIP相同</p></li><li><p>模态内去噪对比学习：</p><ul><li><p>掩模预测和对比学习的结合，在细粒度掩模特征和可见特征(如图像patch、文本token或音频波形特征)之间执行对比损失。</p></li><li><p>引入该任务原因：跨模态对比学习主要关注不同模态的对齐，缺乏对模态内细粒度细节学习的重视，导致下游任务的性能不佳。</p></li><li><p>模态内去噪对比学习应用于5种类型的数据:图像、音频、文本、图像-文本对和音频-文本对。</p></li></ul><img src="https://s2.loli.net/2023/07/17/QyV7leKRP4vuUYE.png" alt="image.png" style="zoom: 67%;" /></li></ul><h2 id="训练加速-x2F-内存优化策略"><a href="#训练加速-x2F-内存优化策略" class="headerlink" title="训练加速&#x2F;内存优化策略"></a>训练加速&#x2F;内存优化策略</h2><ul><li>xformers库： memory-efficient attention</li><li>checkpoint机制：节省内存，以更大的batchsize训练模型</li><li>Flash Attention库：Fused LayerNorm</li><li>nvFuser：融合dropout、LayerScale、随机深度和残差和操作，可以带来额外的速度提升</li><li>float16精度训练</li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>多模态</tag>
      
      <tag>自监督学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文笔记1 - VLMo</title>
    <link href="/2023/07/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B01-VLMo/"/>
    <url>/2023/07/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B01-VLMo/</url>
    
    <content type="html"><![CDATA[<h1 id="VLMo-Unified-Vision-Language-Pre-Training-with-Mixture-of-Modality-Experts"><a href="#VLMo-Unified-Vision-Language-Pre-Training-with-Mixture-of-Modality-Experts" class="headerlink" title="VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts"></a>VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts</h1><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><ul><li>一种图文多模态预训练方法，经过预训练的VLMO可作为<strong>视觉语言分类任务</strong>（Vision-Language Classification ）的融合编码器或<strong>图像文本检索</strong>（Vision-Language Retrieval）的双重编码器进行微调。</li></ul><h2 id="训练策略"><a href="#训练策略" class="headerlink" title="训练策略"></a>训练策略</h2><p>以往的两种主流图文预训练架构：</p><p>①对比学习：双编码器，计算图像和文本的相似度，这种方法对检索任务非常有效，但图像和文本之间简单的浅层交互不足以处理复杂的VL分类任务。（CLIP特征的局限性，DALL·E 2论文中也提到了这个问题）</p><p>②融合编码器（fusion encoder）：联合编码所有可能的图像-文本对，以计算检索任务的相似性得分。在VL分类任务中表现出色，但计算量大。</p><p>VLMo模型是这两种预训练策略的结合。</p><h3 id="预训练"><a href="#预训练" class="headerlink" title="预训练"></a>预训练</h3><p>预训练过程分为三阶段，图文数据共享attention层，通过添加不同的type_embedding区分数据类型。</p><p><img src="https://s2.loli.net/2023/07/13/ticYuRIhqJfsOQG.png"></p><ul><li><p>纯图训练，采用beit提出的MIM方法对attention部分和V-FFN部分训练</p></li><li><p>纯文本训练，冻结attention和V-FFN的参数，掩码文本训练L-FFN</p></li><li><p>图像-文本对训练，开放调整所有参数，使用Image-Text Contrast、Masked Language Modeling和Image-Text Matching三个任务进行训练</p><img src="https://s2.loli.net/2023/07/13/jAXD9UfV1r5Yihm.png" alt="image-20230712214851749.png" style="zoom:67%;" /></li></ul><p>优点：除了图像-文本对之外，还有效地利用了大规模纯图像和纯文本数据。在大量纯图像和纯文本数据上进行分阶段预训练有助于VLMO学习更多通用表征。</p><h3 id="微调"><a href="#微调" class="headerlink" title="微调"></a>微调</h3><ul><li>视觉语言检索，用作双编码器</li><li>视觉语言分类，用作融合编码器</li></ul><p><img src="https://s2.loli.net/2023/07/13/eVJjkRx8z5MdlAC.png"></p>]]></content>
    
    
    
    <tags>
      
      <tag>多模态</tag>
      
      <tag>自监督学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2023/07/12/hello-world/"/>
    <url>/2023/07/12/hello-world/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo new <span class="hljs-string">&quot;My New Post&quot;</span><br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo server<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo generate<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo deploy<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
