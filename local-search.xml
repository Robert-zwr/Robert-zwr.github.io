<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>论文笔记3 - LLaVA</title>
    <link href="/2023/07/19/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B03-LLaVA/"/>
    <url>/2023/07/19/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B03-LLaVA/</url>
    
    <content type="html"><![CDATA[<h1 id="LLaVA-Large-Language-and-Vision-Assistant"><a href="#LLaVA-Large-Language-and-Vision-Assistant" class="headerlink" title="LLaVA: Large Language and Vision Assistant"></a>LLaVA: Large Language and Vision Assistant</h1><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>结合了CLIP vision encoder与LLaMA的多模态大模型，具有与GPT-4相似的能力。</p><h2 id="数据集生成"><a href="#数据集生成" class="headerlink" title="数据集生成"></a>数据集生成</h2><p>数据集的形式为图片和针对该图片的多组文本描述（问题-答案对）。</p><ul><li><p>图片：来自COCO数据集</p></li><li><p>图片的文本描述</p><ul><li>问题：对图片内容的提问，在预先设定好的问题表中抽取得到</li><li>答案：不需要人工标注，而是使用ChatGPT&#x2F;GPT-4生成。由于现有的ChatGPT&#x2F;GPT-4不接受图像输入，图像被转化为<strong>captions</strong>+<strong>bounding boxes</strong>的描述方式作为输入。</li></ul><img src="https://s2.loli.net/2023/07/19/tePYxGLwcAmbyTS.png" alt="image.png" style="zoom:80%;" /></li></ul><h2 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h2><img src="https://llava-vl.github.io/images/llava_arch.png" alt="img" style="zoom: 50%;" /><ul><li><p>Vision Encoder：来自CLIP，将图像$X_V$映射到图像特征$Z_V$</p></li><li><p>Projection：仅使用一个投射矩阵W将图像特征$Z_V$映射到语义空间$H_V$，与问题的language embedding（$H_q$）对齐</p><p>作者还探讨了其他可能的连接图像和语言表征的方法：</p><ul><li><strong>gated cross-attention</strong> in Flamingo </li><li><strong>Q-former</strong> in BLIP-2 </li><li><strong>SAM</strong></li></ul></li><li><p>LLM：预测问题答案</p></li></ul><h2 id="训练策略"><a href="#训练策略" class="headerlink" title="训练策略"></a>训练策略</h2><p>训练时，数据集以如下形式输入模型：<br>$$<br>\begin{aligned}<br>&amp; \mathbf{X}<em>{\text {system-message }}\textcolor{green}{\langle\mathrm{STOP}\rangle} \backslash \mathrm{n} \<br>&amp; \text { Human : } \mathbf{X}</em>{\text {instruct }}^1\textcolor{green}{\langle\mathrm{STOP}\rangle} \backslash \mathrm{n} \text { Assistant: } \textcolor{green}{\mathbf{X}<em>{\mathrm{a}}^1\langle\mathrm{STOP}\rangle} \backslash \mathrm{n} \<br>&amp; \text { Human : } \mathbf{X}</em>{\text {instruct }}^2\textcolor{green}{\langle\mathrm{STOP}\rangle} \backslash \mathrm{n} \text { Assistant: } \textcolor{green}{\mathbf{X}_{\mathrm{a}}^2\langle\mathrm{STOP}\rangle} \backslash \mathrm{n} \ldots<br>\end{aligned}<br>$$</p><p>其中：</p><p>$\mathbf{X}_{\text {system-message }}$ &#x3D;  <em>A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human’s questions.</em> </p><p>​&lt;STOP&gt; &#x3D; <em>###</em></p><p>image embedding在对话的开始被输入，此后$\mathbf{X}<em>{\text {instruct}}$均为language embedding：<br>$$<br>\mathbf{X}</em>{\text {instruct }}^t&#x3D; \begin{cases}\text { Random choose }\left[\mathbf{X}<em>{\mathrm{q}}^1, \mathbf{X}</em>{\mathrm{v}}\right] \text { or }\left[\mathbf{X}<em>{\mathrm{v}}, \mathbf{X}</em>{\mathrm{q}}^1\right], &amp; \text { the first turn } t&#x3D;1 \ \mathbf{X}<em>{\mathrm{q}}^t, &amp; \text { the remaining turns } t&gt;1\end{cases}<br>$$<br>训练目标是：将输出文本x最大化似然到真值$X_a$，模型在预测时能看到之前的全部图像与文本：<br>$$<br>p\left(\mathbf{X}</em>{\mathrm{a}} \mid \mathbf{X}<em>{\mathrm{v}}, \mathbf{X}</em>{\text {instruet }}\right)&#x3D;\prod_{i&#x3D;1}^L p_{\boldsymbol{\theta}}\left(x_i \mid \mathbf{X}<em>{\mathrm{v}}, \mathbf{X}</em>{\text {instruct },&lt;i}, \mathbf{X}_{\mathrm{a},&lt;i}\right)<br>$$<br>采用两阶段的训练过程：</p><ul><li>Pre-training for Feature Alignment：冻结Vision Encoder和LLM，只调整W的参数，要求模型对图像做一些简单的描述。这个阶段可以理解为<strong>为固定参数的LLM训练一个兼容的visual tokenizer</strong></li><li>Fine-tuning End-to-End:仅保持Vision Encoder的参数不变，训练W和LLM的参数，针对特定任务微调。</li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>多模态</tag>
      
      <tag>GPT-4</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文笔记2 - ONE-PEACE</title>
    <link href="/2023/07/16/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B02-ONE-PEACE/"/>
    <url>/2023/07/16/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B02-ONE-PEACE/</url>
    
    <content type="html"><![CDATA[<h1 id="ONE-PEACE-EXPLORING-ONE-GENERAL-REPRESENTATION-MODEL-TOWARD-UNLIMITED-MODALITIES"><a href="#ONE-PEACE-EXPLORING-ONE-GENERAL-REPRESENTATION-MODEL-TOWARD-UNLIMITED-MODALITIES" class="headerlink" title="ONE-PEACE: EXPLORING ONE GENERAL REPRESENTATION MODEL TOWARD UNLIMITED MODALITIES"></a>ONE-PEACE: EXPLORING ONE GENERAL REPRESENTATION MODEL TOWARD UNLIMITED MODALITIES</h1><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>一种多模态预训练方法，可扩展至无限种模态，在不使用任何视觉或语言预训练模型进行初始化的情况下，ONE-PEACE在广泛的单模态和多模态任务中取得了领先的结果。</p><p>ONE-PEACE实际训练了图像、文本、语音三个模态，其中文本为中心模态，以对齐其他两个模态。</p><h2 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h2><img src="https://s2.loli.net/2023/07/16/yjCSwJXHzTYOrDZ.png" style="zoom:67%;" /><ul><li><p>Modality Adapters（对应ViT中的patch_embed层)</p><ul><li><p>Vision Adapter：使用分层MLP（hMLP，<a href="https://zhuanlan.zhihu.com/p/538015554">来源</a>）改进patch_embed层以更好地适应MIM，hMLP等价于三个卷积层（4x4 conv -&gt; 2x2 conv -&gt; 2x2 conv）级联，不同patch之间没有信息交互，和原生ViT的处理（一个stride&#x3D;16且kernel_size&#x3D;16的卷积层）兼容。</p><img src="https://pic1.zhimg.com/80/v2-bf75fdf36acfff930e91372e4d89dec8_1440w.webp" alt="img" style="zoom: 80%;" /></li><li><p>Audio Adapter：</p><ul><li><p>使用wav2vec 2.0中的CNN特征提取器得到audio embeddings。wav2vec 2.0是音频自监督表征学习的经典方法，其使用Z（audio embeddings）”量化“得到的Q，与对Z mask+transformer得到的C进行对比学习，从而达到预训练的目的。</p><p>这里的”量化“不同于模型压缩中的量化，而是”乘积量化“，意思是指把原来的向量空间分解为若干个低维向量空间的笛卡尔积，并对分解得到的低维向量空间分别做量化，从而把连续空间量化成有限空间。具体为把原来连续的特征空间假设是d维，拆分成G个子空间（codebook），每个子空间维度是d&#x2F;G。然后分别在每个子空间里面聚类（K-mean什么的），一共获得V个中心和其中心特征。每个类别的特征用其中心特征代替。</p><p>结果就是，原来d维的连续空间（有无限种特征表达形式），坍缩成了有限离线的空间[GxV]，其可能的特征种类数就只有G*V个。</p><p>乘积量化巧妙在哪儿：</p><p>乘积量化操作通过将无限的特征表达空间坍缩成有限的离散空间，让特征的鲁棒性更强，不会受少量扰动的影响（只要还在某一类里面，特征都由中心特征来代替）。这个聚类过程也是一个特征提取的过程，让特征的表征能力更强了。</p><p><strong>VQVAE中也有类似的量化操作</strong></p><p><a href="https://zhuanlan.zhihu.com/p/531958632">解读</a>、<a href="https://pytorch.org/audio/stable/tutorials/speech_recognition_pipeline_tutorial.html">Pytorch tutorial</a></p><p><img src="https://pic4.zhimg.com/80/v2-825dd85acfbb13e5dee6d817b523a7d3_1440w.webp" alt="img"></p></li><li><p>Audio Adapter没有使用绝对位置嵌入，而是使用卷积层提取相对位置信息并将其添加到音频嵌入中。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">x_conv = self.pos_conv(x.transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>))<br>  x_conv = x_conv.transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>)<br>  x = x + x_conv<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">(encoder): TransformerEncoder(<br>    (pos_conv): Sequential(<br>      (<span class="hljs-number">0</span>): Conv1d(<span class="hljs-number">768</span>, <span class="hljs-number">768</span>, kernel_size=(<span class="hljs-number">128</span>,), stride=(<span class="hljs-number">1</span>,), padding=(<span class="hljs-number">64</span>,), groups=<span class="hljs-number">16</span>)<br>      (<span class="hljs-number">1</span>): SamePad()<br>      (<span class="hljs-number">2</span>): GELU(approximate=none)<br>    )<br></code></pre></td></tr></table></figure></li></ul></li><li><p>Language Adapter：常规，BPE -&gt; add CLS&#x2F;EOS token -&gt; embedding layer</p></li></ul></li><li><p>Modality Fusion Encoder：每个Transformer层中包含一个共享自注意力层和三个模态前馈网络(ffn)，改进点：</p><ul><li><p>Sub-LayerNorm</p><img src="https://pic2.zhimg.com/80/v2-9a0bc3f22d693ec6235336da81dcc929_1440w.webp" alt="img" style="zoom:50%;" /></li><li><p>GeGLU Activation Function：FFN的中间维数设置为嵌入维数的4倍，这与PaLM的做法一致</p></li><li><p>Relative Position Bias (RPB，相对位置偏差)：添加到自注意力层中的<a href="mailto:&#113;&#x40;&#x6b;&#x2e;&#x54;">&#113;&#x40;&#x6b;&#x2e;&#x54;</a>中，文本和音频引入1D RPB，图像引入2D RPB。在预训练阶段，不同自注意层的相对位置偏差是共享的。在微调阶段，解耦每个自注意层的相对位置偏差，并让它们继承预训练的相对偏差的权重。</p><p><strong>EVA02代码中有RPB的代码实现</strong></p><p><a href="https://www.zhihu.com/tardis/zm/art/577855860?source_id=1005">RPB解读</a></p></li><li><p>LayerScale：在加入残差之前，我们将每一层(attn层和FFN层)的输出乘以一个可学习的对角矩阵。</p></li></ul></li></ul><h2 id="预训练任务"><a href="#预训练任务" class="headerlink" title="预训练任务"></a>预训练任务</h2><ul><li><p>跨模态对比学习：与CLIP相同</p></li><li><p>模态内去噪对比学习：</p><ul><li><p>掩模预测和对比学习的结合，在细粒度掩模特征和可见特征(如图像patch、文本token或音频波形特征)之间执行对比损失。</p></li><li><p>引入该任务原因：跨模态对比学习主要关注不同模态的对齐，缺乏对模态内细粒度细节学习的重视，导致下游任务的性能不佳。</p></li><li><p>模态内去噪对比学习应用于5种类型的数据:图像、音频、文本、图像-文本对和音频-文本对。</p></li></ul><img src="https://s2.loli.net/2023/07/17/QyV7leKRP4vuUYE.png" alt="image.png" style="zoom: 67%;" /></li></ul><h2 id="训练加速-x2F-内存优化策略"><a href="#训练加速-x2F-内存优化策略" class="headerlink" title="训练加速&#x2F;内存优化策略"></a>训练加速&#x2F;内存优化策略</h2><ul><li>xformers库： memory-efficient attention</li><li>checkpoint机制：节省内存，以更大的batchsize训练模型</li><li>Flash Attention库：Fused LayerNorm</li><li>nvFuser：融合dropout、LayerScale、随机深度和残差和操作，可以带来额外的速度提升</li><li>float16精度训练</li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>多模态</tag>
      
      <tag>自监督学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文笔记1 - VLMo</title>
    <link href="/2023/07/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B01-VLMo/"/>
    <url>/2023/07/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B01-VLMo/</url>
    
    <content type="html"><![CDATA[<h1 id="VLMo-Unified-Vision-Language-Pre-Training-with-Mixture-of-Modality-Experts"><a href="#VLMo-Unified-Vision-Language-Pre-Training-with-Mixture-of-Modality-Experts" class="headerlink" title="VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts"></a>VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts</h1><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><ul><li>一种图文多模态预训练方法，经过预训练的VLMO可作为<strong>视觉语言分类任务</strong>（Vision-Language Classification ）的融合编码器或<strong>图像文本检索</strong>（Vision-Language Retrieval）的双重编码器进行微调。</li></ul><h2 id="训练策略"><a href="#训练策略" class="headerlink" title="训练策略"></a>训练策略</h2><p>以往的两种主流图文预训练架构：</p><p>①对比学习：双编码器，计算图像和文本的相似度，这种方法对检索任务非常有效，但图像和文本之间简单的浅层交互不足以处理复杂的VL分类任务。（CLIP特征的局限性，DALL·E 2论文中也提到了这个问题）</p><p>②融合编码器（fusion encoder）：联合编码所有可能的图像-文本对，以计算检索任务的相似性得分。在VL分类任务中表现出色，但计算量大。</p><p>VLMo模型是这两种预训练策略的结合。</p><h3 id="预训练"><a href="#预训练" class="headerlink" title="预训练"></a>预训练</h3><p>预训练过程分为三阶段，图文数据共享attention层，通过添加不同的type_embedding区分数据类型。</p><p><img src="https://s2.loli.net/2023/07/13/ticYuRIhqJfsOQG.png"></p><ul><li><p>纯图训练，采用beit提出的MIM方法对attention部分和V-FFN部分训练</p></li><li><p>纯文本训练，冻结attention和V-FFN的参数，掩码文本训练L-FFN</p></li><li><p>图像-文本对训练，开放调整所有参数，使用Image-Text Contrast、Masked Language Modeling和Image-Text Matching三个任务进行训练</p><img src="https://s2.loli.net/2023/07/13/jAXD9UfV1r5Yihm.png" alt="image-20230712214851749.png" style="zoom:67%;" /></li></ul><p>优点：除了图像-文本对之外，还有效地利用了大规模纯图像和纯文本数据。在大量纯图像和纯文本数据上进行分阶段预训练有助于VLMO学习更多通用表征。</p><h3 id="微调"><a href="#微调" class="headerlink" title="微调"></a>微调</h3><ul><li>视觉语言检索，用作双编码器</li><li>视觉语言分类，用作融合编码器</li></ul><p><img src="https://s2.loli.net/2023/07/13/eVJjkRx8z5MdlAC.png"></p>]]></content>
    
    
    
    <tags>
      
      <tag>多模态</tag>
      
      <tag>自监督学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
