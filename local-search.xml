<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>3倍加速LLM推理——推测解码（Speculative Decoding）</title>
    <link href="/2024/07/24/%E6%8E%A8%E6%B5%8B%E8%A7%A3%E7%A0%81%EF%BC%88Speculative-Decoding%EF%BC%89/"/>
    <url>/2024/07/24/%E6%8E%A8%E6%B5%8B%E8%A7%A3%E7%A0%81%EF%BC%88Speculative-Decoding%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<p>推测解码（SpeculativeDecoding，也有人译为“投机采样”，个人认为意译为“拒绝采样”更好一些）是Google[1]和DeepMind[2]在2022年同时发现的大模型推理加速方法。它可以在生成效果<strong>无损</strong>的前提下，获得<strong>3倍</strong>以上的<strong>加速</strong>比。GPT-4泄密报告也提到OpenAI线上模型推理使用了它。</p><p><img src="https://pic2.zhimg.com/v2-f112b680339b4d6add939f6cae1b8be5_r.jpg" alt="preview" style="zoom:33%;" /></p><h2id="基本思想小模型打草稿大模型纠正">基本思想：小模型打草稿，大模型纠正</h2><ul><li><strong>打草稿</strong>：SpeculativeDecoding引入了一个与原本LLM结构相同但参数量小得多的近似模型。推理过程中，由小模型先用自回归的方法串行生成<spanclass="math inline">\(\gamma\)</span>个token；</li><li><strong>纠正</strong>：再将这<spanclass="math inline">\(\gamma\)</span>个token一起送入大模型中进行推理（验证），对于小模型输出的某个token<span class="math inline">\(x_{i}\)</span>，若大模型预测的概率<spanclass="math inline">\(p(x_i)\)</span>大于小模型预测的概率<spanclass="math inline">\(q(x_i)\)</span>,则接受<spanclass="math inline">\(x_i\)</span>作为当前位置的输出；否则，大模型就以<spanclass="math inline">\(1-\frac{p(x_i)}{q(x_i)}\)</span>的概率概率拒绝<spanclass="math inline">\(x_i\)</span>作为当前位置的输出，并在<spanclass="math inline">\(p(x)&gt;q(x)\)</span>的区域重新采样<spanclass="math inline">\(x_j\)</span>作为纠正后的输出。</li></ul><p><img src="../images/figure1.png" /></p><h2 id="加速原理">加速原理：</h2><p>在SpeculativeDecoding中，串行输出token的任务主要由小模型承担了，大模型的“纠正”过程由于可以一次并行处理<spanclass="math inline">\(\gamma\)</span>个token，前向推理的次数大大减少。（与自回归的LLM可以高效并行训练的原理相同）</p><h2id="无偏估计为什么说speculative-decoding的加速过程是无损的">无偏估计：为什么说SpeculativeDecoding的加速过程是“无损”的</h2><p>论文[1]在附录A1.1中证明了通过从<spanclass="math inline">\(p(x)\)</span>和<spanclass="math inline">\(q(x)\)</span>进行SpeculativeDecoding所得到的token的分布与仅从<spanclass="math inline">\(p(x)\)</span>进行采样所得到的token的分布是相同的。有趣的是，这一结论对于任意分布的<spanclass="math inline">\(p(x)\)</span>和<spanclass="math inline">\(q(x)\)</span>均成立。</p><p><font color=red> <strong>前提</strong>： </font></p><p>Speculative Decoding的过程以以下方法进行：</p><ul><li><p>若<spanclass="math inline">\(p(x&#39;)&gt;q(x&#39;)\)</span>,则模型以1的概率接受<spanclass="math inline">\(x&#39;\)</span>作为最终解码结果；若<spanclass="math inline">\(p(x&#39;)&lt;q(x&#39;)\)</span>,则模型以<spanclass="math inline">\(\frac{p(x&#39;)}{q(x&#39;)}\)</span>的概率接受<spanclass="math inline">\(x&#39;\)</span>作为最终解码结果；即小模型输出的<spanclass="math inline">\(x&#39;\)</span>被接受的概率为<spanclass="math inline">\(\min(1,\frac{p(x&#39;)}{q(x&#39;)})\)</span>。</p></li><li><p>若小模型输出被拒绝，则以归一化后的<spanclass="math inline">\(p(x)-q(x)\)</span>作为概率分布（记作<spanclass="math inline">\(p&#39;(x)\)</span>），在<spanclass="math inline">\(p(x)&gt;q(x)\)</span>的区域重新采样得到作为纠正后的输出。有<spanclass="math inline">\(p^{\prime}(x)=\operatorname{norm}(\max (0,p(x)-q(x)))=\frac{p(x)-\min (q(x),p(x))}{\sum_{x^{\prime}}\left(p\left(x^{\prime}\right)-\min\left(q\left(x^{\prime}\right),p\left(x^{\prime}\right)\right)\right)}=\frac{p(x)-\min (q(x),p(x))}{1-\sum_{x^{\prime}}\min \left(q\left(x^{\prime}\right),p\left(x^{\prime}\right)\right)}\)</span></p></li></ul><p><font color=red> <strong>证明</strong>： </font></p><p><spanclass="math inline">\(P\left(x=x^{\prime}\right)=P\left(\right.\)</span>guess accepted, <spanclass="math inline">\(\left.x=x^{\prime}\right)+P\left(\right.\)</span>guess rejected, <spanclass="math inline">\(\left.x=x^{\prime}\right)\)</span></p><p>其中，</p><p><span class="math inline">\(P\left(\right.\)</span> guess accepted,<spanclass="math inline">\(\left.x=x^{\prime}\right)=q\left(x^{\prime}\right)\min \left(1,\frac{p\left(x^{\prime}\right)}{q\left(x^{\prime}\right)}\right)=\min\left(q\left(x^{\prime}\right),p\left(x^{\prime}\right)\right)\)</span></p><p>小模型的输出被接受的概率<span class="math inline">\(\beta=E_{x \simq(x)} \begin{cases}1 &amp; q(x) \leq p(x) \\ \frac{p(x)}{q(x)} &amp;q(x)&gt;p(x)\end{cases}=E_{x \sim q(x)} \min \left(1,\frac{p(x)}{q(x)}\right)=\sum_{x&#39;} \min (p(x&#39;),q(x&#39;))\)</span>，代入<spanclass="math inline">\(p&#39;(x)\)</span>的表达式，有<spanclass="math inline">\(p&#39;(x)=\frac{p(x)-\min (q(x),p(x))}{1-\beta}\)</span></p><p>则<span class="math inline">\(P\left(\right.\)</span> guess rejected,<span class="math inline">\(\left.x=x^{\prime}\right)=(1-\beta)p^{\prime}\left(x^{\prime}\right)=p\left(x^{\prime}\right)-\min\left(q\left(x^{\prime}\right),p\left(x^{\prime}\right)\right)\)</span></p><p>故：<spanclass="math inline">\(P\left(x=x^{\prime}\right)=p(x&#39;)\)</span></p><p>证毕，As desired!</p>]]></content>
    
    
    
    <tags>
      
      <tag>LLM</tag>
      
      <tag>HPC</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>矩阵求导术</title>
    <link href="/2024/06/05/%E7%9F%A9%E9%98%B5%E6%B1%82%E5%AF%BC%E6%9C%AF/"/>
    <url>/2024/06/05/%E7%9F%A9%E9%98%B5%E6%B1%82%E5%AF%BC%E6%9C%AF/</url>
    
    <content type="html"><![CDATA[<p>自定义Triton算子，需要在实现反向传播时给出相应的梯度计算方法，梯度表达式的推导过程涉及矩阵求导的运算法则。</p><p>下面以<strong><ahref="https://github.com/sustcsonglin/flash-linear-attention">flash-linear-attention</a></strong>为例，介绍一下flash-linear-attention的实现方法，顺便推导一下梯度的计算过程。</p><h2 id="forward">Forward</h2><ul><li><p><strong>标准attention:</strong></p><ul><li><p>train：并行，高效 <span class="math display">\[\begin{align*}\rm Q, \rm K, \rm V &amp;= \rm XW_Q, \rm XW_K, \rm XW_V, \\  \rm O&amp;= \ softmax\big((\rm Q \rm K^\intercal) \odot \rm M \big) \rm V,%\end{align*}\]</span> where <span class="math inline">\(\rm X \in R^{L \timesd}\)</span>，<span class="math inline">\(W_Q, W_K, W_V \in R^{d \timesd}\)</span>， <span class="math inline">\(\rm M \in \{-\infty,1\}^{L\times L}\)</span> , <span class="math inline">\(\rm M_{ij}=1\)</span>if <span class="math inline">\(i\ge j\)</span> and <spanclass="math inline">\(\rm M_{ij}=-\infty\)</span> if <spanclass="math inline">\(i&lt;j\)</span>. (Here we assume a singleattention head for simplicity.)</p></li><li><p>inference：串行，kv cache <span class="math display">\[\begin{align*}q_t, \ k_t, \ v_t &amp;= x_t W_Q, \  x_t W_K, \ x_t W_V, \\ o_t &amp;=\frac{\sum_{i=1}^{t} \exp(q_t  k_i^\intercal)v_i}{\sum_{i =1} ^{t}\exp(q_t  k_i^\intercal)},\end{align*}\]</span></p></li></ul></li><li><p><strong>Linear attention</strong></p></li></ul><p>replace <span class="math inline">\(\exp(q_t k_i^\intercal)\)</span>with a kernel <span class="math inline">\(k(x, y) = \langle\phi(x),\phi(y)\rangle\)</span>)，since we have: <span class="math display">\[  \begin{align*}   o_t &amp;= \frac{\sum_{i=1}^{ t}\phi(q_t)\phi(k_i)^\intercalv_i}{\sum_{i=1}^{t} \phi(q_t)\phi(k_i)^\intercal  }     = \frac{\phi(q_t)   \sum_{i=1}^{t}\phi(k_i)^\intercal v_i}{\phi(q_t)\sum_{i=1}^{t}\phi(k_i)^\intercal}.   %  \end{align*}  \]</span> Letting <span class="math inline">\(\rmS_t=\sum_{i=1}^{t}\phi(k_i)^\intercal v_i\)</span> and <spanclass="math inline">\(z_t=\sum_{i=1}^{t}\phi(k_i)^\intercal\)</span>where <span class="math inline">\(\rm S_t \in \mathbb{R}^{d\times d},z_t \in \mathbb{R}^{d\times 1}\)</span>, we can rewrite the above as anRNN: <span class="math display">\[  \begin{align*}  \rm S_t = \rm S_{t-1} + \phi(k_t)^\intercal v_t, \ z_t = z_{t-1} +\phi(k_t)^\intercal, \ o_t = \frac{\phi(q_t) \rm S_t}{ \phi(q_t) z_t}.  \end{align*}  \]</span> recent work has found that a linear kernel (i.e., setting<span class="math inline">\(\phi\)</span> to be the identity) without anormalizer works well in practice: <span class="math display">\[  \begin{align}  &amp; \rm S_t = \rm S_{t-1} + k_t^\intercal v_t, \quad o_t = q_t \rmS_t.  \end{align}  \]</span></p><ul><li><p><strong>分块并行形式</strong></p><p>for <span class="math inline">\(i \in [0, 1, \dots\frac{L}{C}-1]\)</span>： <span class="math display">\[\rm S_{[i+1]} = \rm S_{[i]} + \underbrace{\sum_{j=iC + 1}^{(i+1)C}k_{j}^\intercal v_{j}}_{\rm K^\intercal_{[i]}\rm V_{[i]}} \quad\hspace{1mm} \in \mathbb{R}^{d\times d}.\]</span></p><p><span class="math display">\[\rm O_{[i+1]} = \underbrace{\rm Q_{[i+1]}\rmS_{[i]}}_{\text{inter-chunk}: \rm O^\text{inter}_{[i+1]}} +\underbrace{\big((\rm Q_{[i+1]}\rm K_{[i+1]}^{\intercal})\odot\rmM\big)\rm V_{[i+1]}}_{\text{intra-chunk}: \rm O^{\text{intra}}_{[i+1]}},\]</span></p><p>算法流程如下：</p><figure><img src="https://s2.loli.net/2024/06/06/y9MPioW3N1UsndO.png"alt="image.png" /><figcaption aria-hidden="true">image.png</figcaption></figure></li></ul><h2 id="backward">Backward</h2><p><img src="https://s2.loli.net/2024/06/06/BfCuxnYrzpHZ4JV.png"alt="2024-06-06 163230.png" /> <span class="math display">\[\rm O = \rm Q_{[n]} \rm S +(\rm Q_{[n]} \rm K_{[n]}^\intercal \odot\rmM)\rm V_{[n]}\]</span></p><p><span class="math display">\[\rm S = \rm S + \rm K_{[n]}^\intercal \rm V_{[n]}\]</span></p><p><strong>输入</strong>：<span class="math inline">\(\nabla O\)</span>,<strong>输出</strong>：<span class="math inline">\(\nablaQ\)</span>,<span class="math inline">\(\nabla K\)</span>,<spanclass="math inline">\(\nabla V\)</span></p><p><strong>S</strong>, <strong>Q</strong>, <strong>K</strong>,<strong>V</strong>均有两个输出节点（即图中有两个向外的箭头），因此它们的梯度均由两部分构成：</p><ul><li><p><strong>S</strong>:</p><p><font color=red><strong><em>引理1</em></strong>： </font><strong>Y、A、X、B</strong>均为矩阵，若<strong>Y</strong>=<strong>AXB</strong>，则<spanclass="math display">\[\nabla X = A^\intercal \nabla YB^\intercal\]</span></p><p>根据该引理，我们有：</p><p><span class="math display">\[\nabla S_{①} \ = \rm Q_{[n]}^\intercal\nabla O\]</span></p><p><span class="math display">\[\nabla S_{②} \ = \nablaS^{&#39;}(下一级S的梯度)\]</span></p><p><span class="math display">\[\nabla S = \nabla S_{①} + \nabla S_{②} \= \rm Q_{[n]}^\intercal \nabla O + \nabla S\]</span></p></li><li><p><strong>Q</strong>:</p><p><span class="math inline">\(\nabla Q_{③} \ = \nabla O \ \rmS^\intercal\)</span></p><p><font color=red> <strong><em>引理2</em></strong>：</font>矩阵链式求导的一般方法：借助矩阵的迹（trace）运算。</p><p>迹的定义：方阵的主对角线元素之和。</p><p>由迹的定义易得：</p><ul><li>标量套上迹：<span class="math inline">\(a = tr(a)\)</span></li><li>转置： <span class="math inline">\(tr(A) = tr(A^T)\)</span></li><li>线性： <span class="math inline">\(tr(A±B) = tr(A) +tr(B)\)</span></li><li>矩阵乘法交换律：<span class="math inline">\(tr(AB)=tr(BA) =\sum_{i,j}A_{ij}B_{ij}^T = \sum_{i,j}A_{ij}B_{ij}\)</span> ，即A和<spanclass="math inline">\(B^T\)</span>（或B和<spanclass="math inline">\(A^T\)</span>）的内积</li><li>逐元素相乘：<span class="math inline">\(tr(A^T(B \odot C)) = tr((A\odot B)^T C) = \sum_{i,j}A_{ij}B_{ij}C_{ij}\)</span></li></ul><p><font color=red> <strong>矩阵链式求导</strong>： </font><spanclass="math inline">\(df = tr(\nabla Y^T dY) = tr(\nabla X^TdX)\)</span></p><p>故有：<span class="math display">\[tr(\nabla O^T dO) \\=tr\big(\nabla O^T (dQ_{[n]}K_{[n]}^T \odot M)V_{[n]}\big)\\=tr\big(V_{[n]}\nabla O^T (dQ_{[n]}K_{[n]}^T \odotM)\big)\\=tr\big((\nabla OV_{[n]}^T \odotM)^TdQ_{[n]}K_{[n]}^T\big)\\=tr\big(K_{[n]}^T(\nabla OV_{[n]}^T \odotM)^TdQ_{[n]}\big)\\=tr\big(((\nabla OV_{[n]}^T \odotM)K_{[n]})^TdQ_{[n]}\big)\]</span></p><p>上述推导过程主要使用了迹的矩阵乘法交换律和逐元素相乘律，对照链式求导公式可得，<spanclass="math display">\[\nabla Q_{④} = (\nabla OV_{[n]}^T \odotM)K_{[n]}\]</span></p><p><span class="math display">\[\nabla Q_{[n]} = \nabla Q_{③} + \nablaQ_{④} \ = \nabla O \ \rm S^\intercal + (\nabla OV_{[n]}^T \odotM)K_{[n]}\]</span></p></li><li><p><strong>K、V</strong>：类似方法可得：</p><p><span class="math display">\[\nabla K_{[n]} = \nabla K_{⑤} + \nablaK_{⑥} \ = V_{[n]}\nabla S^T + (V_{[n]}\nabla O^T \odotM^T)Q_{[n]}\]</span></p><p><span class="math display">\[\nabla V_{[n]} = \nabla V_{⑦} + \nablaV_{⑧} \ = K_{[n]}\nabla S + (\rm Q_{[n]} \rm K_{[n]}^T \odot\rm M)^T\nabla O\]</span></p></li></ul><p>具体算法流程：</p><figure><img src="https://s2.loli.net/2024/06/06/CPlvUVz6GnQhuas.png"alt="image.png" /><figcaption aria-hidden="true">image.png</figcaption></figure><h2 id="参考文献">参考文献：</h2><ul><li><a href="https://zhuanlan.zhihu.com/p/24709748">矩阵求导术（上） -知乎 (zhihu.com)</a></li><li>GLA：<a href="https://arxiv.org/abs/2312.06635">paper</a>, <ahref="https://github.com/berlino/gated_linear_attention">code</a></li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>Triton</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文笔记4 - Flamingo</title>
    <link href="/2023/07/19/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B04-Flamingo/"/>
    <url>/2023/07/19/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B04-Flamingo/</url>
    
    <content type="html"><![CDATA[<h1id="flamingo-a-visual-language-model-for-few-shot-learning">Flamingo: aVisual Language Model for Few-Shot Learning</h1><h2 id="概述">概述</h2><ul><li>最先进的few-shot视觉问答注意力网络，接受灵活的图像/文本输入</li><li>利用cross attention实现图像压缩与图文信息交互</li><li>通过设置prompt，大模型+少样本打败精调。</li></ul><p><img src="https://s2.loli.net/2023/07/22/5x8X2ECartFBiSk.png" style="zoom: 80%;" /></p><h2 id="模型结构">模型结构</h2><h3 id="怎样将图像和文本一起输入llm">怎样将图像和文本一起输入LLM？</h3><p>将图像数据从2D转化为1D，以输入Transformer，作者除采用ViT中常见的patch_embedding（一个预训练的Resnet）外，还用一个Transformer模块对imagepatch的token数量进行了压缩。</p><p>压缩的方法就是引入一个PerceiverResampler模块，核心是做attention，如下图所示，用R个可学习的token作为Query，图像编码<spanclass="math inline">\(X_f\)</span>和R个token的拼接作为K和V（拼接后效果略好），这样attention下来输出也是R个token,从而将图像token数量压缩到R个。</p><p><imgsrc="https://pic1.zhimg.com/80/v2-b5a9c3d3bccdfe01aa066408a9db057c_720w.webp" /></p><h3 id="如何利用预训练的llm">如何利用预训练的LLM？</h3><p>作者从一个预训练好的70B语言模型Chinchilla开始加入图像信息，考虑到保留之前LM学到的知识，训练时需要把LM的参数冻住，因此提出了<strong>GatedCross attention</strong>机制来融合图像和文本。</p><p>具体来说，还是利用attention机制，如下图所示，把文本编码当作Q，图像编码当作K和V，这样输出的维度还是跟之前文本的一样。之后还是正常过FFW。同时为了提升稳定性和效果，在attention和FFW之后都增加了tanh门控，先开始是0，之后逐步增加。</p><p><strong>这个过程可以理解为以文本输入为载体，将图像信息嵌入到文本信息之中</strong></p><figure><imgsrc="https://pic3.zhimg.com/80/v2-76e0ce3eb141ce5512a2e5af441221e2_720w.webp"alt="y: visually informed language features" /><figcaption aria-hidden="true">y: visually informed languagefeatures</figcaption></figure><p>但为了减少参数量，这个机制不一定每层都有（但每层都添加效果更好），具体增加的策略如下：</p><p><imgsrc="https://pic2.zhimg.com/80/v2-560954ec7610438b74d788c7ade53449_720w.webp" /></p><p>同时为了减少不相关的图像噪声，提高训练的稳定性，增加mask策略使得在cross-attention时每段文字只能关注到它之前的一个图像。</p><p>但cross-attention之后的self-attention还是保持LM的causalattention，可以保证在预测输出时会关注到以前所有的文字和图像。</p><h3 id="如何构造训练数据">如何构造训练数据？</h3><p>需要的训练数据为图像文本对，在大量网页上爬取得到M3W数据集。</p>]]></content>
    
    
    
    <tags>
      
      <tag>多模态</tag>
      
      <tag>gpt-4</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文笔记3 - LLaVA</title>
    <link href="/2023/07/19/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B03-LLaVA/"/>
    <url>/2023/07/19/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B03-LLaVA/</url>
    
    <content type="html"><![CDATA[<h1 id="llava-large-language-and-vision-assistant">LLaVA: Large Languageand Vision Assistant</h1><h2 id="概述">概述</h2><p>结合了CLIP visionencoder与LLaMA的多模态大模型，具有与GPT-4相似的能力。</p><h2 id="数据集生成">数据集生成</h2><p>数据集的形式为图片和针对该图片的多组文本描述（问题-答案对）。</p><ul><li><p>图片：来自COCO数据集</p></li><li><p>图片的文本描述</p><ul><li>问题：对图片内容的提问，在预先设定好的问题表中抽取得到</li><li>答案：不需要人工标注，而是使用ChatGPT/GPT-4生成。由于现有的ChatGPT/GPT-4不接受图像输入，图像被转化为<strong>captions</strong>+<strong>boundingboxes</strong>的描述方式作为输入。</li></ul><p><img src="https://s2.loli.net/2023/07/19/tePYxGLwcAmbyTS.png" alt="image.png" style="zoom:80%;" /></p></li></ul><h2 id="模型结构">模型结构</h2><p><img src="https://llava-vl.github.io/images/llava_arch.png" alt="img.png" style="zoom: 50%;" /></p><ul><li><p>Vision Encoder：来自CLIP，将图像<spanclass="math inline">\(X_V\)</span>映射到图像特征<spanclass="math inline">\(Z_V\)</span></p></li><li><p>Projection：仅使用一个投射矩阵W将图像特征<spanclass="math inline">\(Z_V\)</span>映射到语义空间<spanclass="math inline">\(H_V\)</span>，与问题的language embedding（<spanclass="math inline">\(H_q\)</span>）对齐</p><p>作者还探讨了其他可能的连接图像和语言表征的方法：</p><ul><li><strong>gated cross-attention</strong> in Flamingo</li><li><strong>Q-former</strong> in BLIP-2</li><li><strong>SAM</strong></li></ul></li><li><p>LLM：预测问题答案</p></li></ul><h2 id="训练策略">训练策略</h2><p>训练时，数据集以如下形式输入模型： <span class="math display">\[\begin{aligned}&amp; \mathbf{X}_{\text {system-message}}\textcolor{green}{\langle\mathrm{STOP}\rangle} \backslash \mathrm{n}\\&amp; \text { Human : } \mathbf{X}_{\text {instruct}}^1\textcolor{green}{\langle\mathrm{STOP}\rangle} \backslash \mathrm{n}\text { Assistant: }\textcolor{green}{\mathbf{X}_{\mathrm{a}}^1\langle\mathrm{STOP}\rangle}\backslash \mathrm{n} \\&amp; \text { Human : } \mathbf{X}_{\text {instruct}}^2\textcolor{green}{\langle\mathrm{STOP}\rangle} \backslash \mathrm{n}\text { Assistant: }\textcolor{green}{\mathbf{X}_{\mathrm{a}}^2\langle\mathrm{STOP}\rangle}\backslash \mathrm{n} \ldots\end{aligned}\]</span></p><p>其中：</p><p><span class="math inline">\(\mathbf{X}_{\text {system-message}}\)</span> = <em>A chat between a curious human and an artificialintelligence assistant. The assistant gives helpful, detailed, andpolite answers to the human’s questions.</em></p><p>​ &lt;STOP&gt; = <em>###</em></p><p>image embedding在对话的开始被输入，此后<spanclass="math inline">\(\mathbf{X}_{\text {instruct}}\)</span>均为languageembedding： <span class="math display">\[\mathbf{X}_{\text {instruct }}^t= \begin{cases}\text { Random choose}\left[\mathbf{X}_{\mathrm{q}}^1, \mathbf{X}_{\mathrm{v}}\right] \text {or }\left[\mathbf{X}_{\mathrm{v}}, \mathbf{X}_{\mathrm{q}}^1\right],&amp; \text { the first turn } t=1 \\ \mathbf{X}_{\mathrm{q}}^t, &amp;\text { the remaining turns } t&gt;1\end{cases}\]</span> 训练目标是：将输出文本x最大化似然到真值<spanclass="math inline">\(X_a\)</span>，模型在预测时能看到之前的全部图像与文本：<span class="math display">\[p\left(\mathbf{X}_{\mathrm{a}} \mid \mathbf{X}_{\mathrm{v}},\mathbf{X}_{\text {instruet }}\right)=\prod_{i=1}^Lp_{\boldsymbol{\theta}}\left(x_i \mid \mathbf{X}_{\mathrm{v}},\mathbf{X}_{\text {instruct },&lt;i},\mathbf{X}_{\mathrm{a},&lt;i}\right)\]</span> 采用两阶段的训练过程：</p><ul><li>Pre-training for Feature Alignment：冻结VisionEncoder和LLM，只调整W的参数，要求模型对图像做一些简单的描述。这个阶段可以理解为<strong>为固定参数的LLM训练一个兼容的visualtokenizer</strong></li><li>Fine-tuning End-to-End:仅保持VisionEncoder的参数不变，训练W和LLM的参数，针对特定任务微调。</li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>多模态</tag>
      
      <tag>GPT-4</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文笔记2 - ONE-PEACE</title>
    <link href="/2023/07/16/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B02-ONE-PEACE/"/>
    <url>/2023/07/16/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B02-ONE-PEACE/</url>
    
    <content type="html"><![CDATA[<h1id="one-peace-exploring-one-general-representation-model-toward-unlimited-modalities">ONE-PEACE:EXPLORING ONE GENERAL REPRESENTATION MODEL TOWARD UNLIMITEDMODALITIES</h1><h2 id="概述">概述</h2><p>一种多模态预训练方法，可扩展至无限种模态，在不使用任何视觉或语言预训练模型进行初始化的情况下，ONE-PEACE在广泛的单模态和多模态任务中取得了领先的结果。</p><p>ONE-PEACE实际训练了图像、文本、语音三个模态，其中文本为中心模态，以对齐其他两个模态。</p><h2 id="模型结构">模型结构</h2><p><img src="https://s2.loli.net/2023/07/16/yjCSwJXHzTYOrDZ.png" style="zoom:67%;" /></p><ul><li><p>Modality Adapters（对应ViT中的patch_embed层)</p><ul><li><p>Vision Adapter：使用分层MLP（hMLP，<ahref="https://zhuanlan.zhihu.com/p/538015554">来源</a>）改进patch_embed层以更好地适应MIM，hMLP等价于三个卷积层（4x4conv -&gt; 2x2 conv -&gt; 2x2conv）级联，不同patch之间没有信息交互，和原生ViT的处理（一个stride=16且kernel_size=16的卷积层）兼容。</p><p><img src="https://pic1.zhimg.com/80/v2-bf75fdf36acfff930e91372e4d89dec8_1440w.webp" alt="img" style="zoom: 80%;" /></p></li><li><p>Audio Adapter：</p><ul><li><p>使用wav2vec 2.0中的CNN特征提取器得到audio embeddings。wav2vec2.0是音频自监督表征学习的经典方法，其使用Z（audioembeddings）”量化“得到的Q，与对Zmask+transformer得到的C进行对比学习，从而达到预训练的目的。</p><p>这里的”量化“不同于模型压缩中的量化，而是”乘积量化“，意思是指把原来的向量空间分解为若干个低维向量空间的笛卡尔积，并对分解得到的低维向量空间分别做量化，从而把连续空间量化成有限空间。具体为把原来连续的特征空间假设是d维，拆分成G个子空间（codebook），每个子空间维度是d/G。然后分别在每个子空间里面聚类（K-mean什么的），一共获得V个中心和其中心特征。每个类别的特征用其中心特征代替。</p><p>结果就是，原来d维的连续空间（有无限种特征表达形式），坍缩成了有限离线的空间[GxV]，其可能的特征种类数就只有G*V个。</p><p>乘积量化巧妙在哪儿：</p><p>乘积量化操作通过将无限的特征表达空间坍缩成有限的离散空间，让特征的鲁棒性更强，不会受少量扰动的影响（只要还在某一类里面，特征都由中心特征来代替）。这个聚类过程也是一个特征提取的过程，让特征的表征能力更强了。</p><p><strong>VQVAE中也有类似的量化操作</strong></p><p><a href="https://zhuanlan.zhihu.com/p/531958632">解读</a>、<ahref="https://pytorch.org/audio/stable/tutorials/speech_recognition_pipeline_tutorial.html">Pytorchtutorial</a></p><figure><imgsrc="https://pic4.zhimg.com/80/v2-825dd85acfbb13e5dee6d817b523a7d3_1440w.webp"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure></li><li><p>AudioAdapter没有使用绝对位置嵌入，而是使用卷积层提取相对位置信息并将其添加到音频嵌入中。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">x_conv = self.pos_conv(x.transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>))<br>  x_conv = x_conv.transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>)<br>  x = x + x_conv<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">(encoder): TransformerEncoder(<br>    (pos_conv): Sequential(<br>      (<span class="hljs-number">0</span>): Conv1d(<span class="hljs-number">768</span>, <span class="hljs-number">768</span>, kernel_size=(<span class="hljs-number">128</span>,), stride=(<span class="hljs-number">1</span>,), padding=(<span class="hljs-number">64</span>,), groups=<span class="hljs-number">16</span>)<br>      (<span class="hljs-number">1</span>): SamePad()<br>      (<span class="hljs-number">2</span>): GELU(approximate=none)<br>    )<br></code></pre></td></tr></table></figure></li></ul></li><li><p>Language Adapter：常规，BPE -&gt; add CLS/EOS token -&gt;embedding layer</p></li></ul></li><li><p>Modality FusionEncoder：每个Transformer层中包含一个共享自注意力层和三个模态前馈网络(ffn)，改进点：</p><ul><li><p>Sub-LayerNorm</p><p><img src="https://pic2.zhimg.com/80/v2-9a0bc3f22d693ec6235336da81dcc929_1440w.webp" alt="img" style="zoom:50%;" /></p></li><li><p>GeGLU ActivationFunction：FFN的中间维数设置为嵌入维数的4倍，这与PaLM的做法一致</p></li><li><p>Relative Position Bias(RPB，相对位置偏差)：添加到自注意力层中的q@k.T中，文本和音频引入1DRPB，图像引入2DRPB。在预训练阶段，不同自注意层的相对位置偏差是共享的。在微调阶段，解耦每个自注意层的相对位置偏差，并让它们继承预训练的相对偏差的权重。</p><p><strong>EVA02代码中有RPB的代码实现</strong></p><p><ahref="https://www.zhihu.com/tardis/zm/art/577855860?source_id=1005">RPB解读</a></p></li><li><p>LayerScale：在加入残差之前，我们将每一层(attn层和FFN层)的输出乘以一个可学习的对角矩阵。</p></li></ul></li></ul><h2 id="预训练任务">预训练任务</h2><ul><li><p>跨模态对比学习：与CLIP相同</p></li><li><p>模态内去噪对比学习：</p><ul><li><p>掩模预测和对比学习的结合，在细粒度掩模特征和可见特征(如图像patch、文本token或音频波形特征)之间执行对比损失。</p></li><li><p>引入该任务原因：跨模态对比学习主要关注不同模态的对齐，缺乏对模态内细粒度细节学习的重视，导致下游任务的性能不佳。</p></li><li><p>模态内去噪对比学习应用于5种类型的数据:图像、音频、文本、图像-文本对和音频-文本对。</p></li></ul><p><img src="https://s2.loli.net/2023/07/17/QyV7leKRP4vuUYE.png" alt="image.png" style="zoom: 67%;" /></p></li></ul><h2 id="训练加速内存优化策略">训练加速/内存优化策略</h2><ul><li>xformers库： memory-efficient attention</li><li>checkpoint机制：节省内存，以更大的batchsize训练模型</li><li>Flash Attention库：Fused LayerNorm</li><li>nvFuser：融合dropout、LayerScale、随机深度和残差和操作，可以带来额外的速度提升</li><li>float16精度训练</li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>多模态</tag>
      
      <tag>自监督学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文笔记1 - VLMo</title>
    <link href="/2023/07/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B01-VLMo/"/>
    <url>/2023/07/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B01-VLMo/</url>
    
    <content type="html"><![CDATA[<h1id="vlmo-unified-vision-language-pre-training-with-mixture-of-modality-experts">VLMo:Unified Vision-Language Pre-Training withMixture-of-Modality-Experts</h1><h2 id="概述">概述</h2><ul><li>一种图文多模态预训练方法，经过预训练的VLMO可作为<strong>视觉语言分类任务</strong>（Vision-LanguageClassification）的融合编码器或<strong>图像文本检索</strong>（Vision-LanguageRetrieval）的双重编码器进行微调。</li></ul><h2 id="训练策略">训练策略</h2><p>以往的两种主流图文预训练架构：</p><p>①对比学习：双编码器，计算图像和文本的相似度，这种方法对检索任务非常有效，但图像和文本之间简单的浅层交互不足以处理复杂的VL分类任务。（CLIP特征的局限性，DALL·E2论文中也提到了这个问题）</p><p>②融合编码器（fusionencoder）：联合编码所有可能的图像-文本对，以计算检索任务的相似性得分。在VL分类任务中表现出色，但计算量大。</p><p>VLMo模型是这两种预训练策略的结合。</p><h3 id="预训练">预训练</h3><p>预训练过程分为三阶段，图文数据共享attention层，通过添加不同的type_embedding区分数据类型。</p><p><img src="https://s2.loli.net/2023/07/13/ticYuRIhqJfsOQG.png" /></p><ul><li><p>纯图训练，采用beit提出的MIM方法对attention部分和V-FFN部分训练</p></li><li><p>纯文本训练，冻结attention和V-FFN的参数，掩码文本训练L-FFN</p></li><li><p>图像-文本对训练，开放调整所有参数，使用Image-TextContrast、Masked Language Modeling和Image-TextMatching三个任务进行训练</p><p><img src="https://s2.loli.net/2023/07/13/jAXD9UfV1r5Yihm.png" alt="image-20230712214851749.png" style="zoom:67%;" /></p></li></ul><p>优点：除了图像-文本对之外，还有效地利用了大规模纯图像和纯文本数据。在大量纯图像和纯文本数据上进行分阶段预训练有助于VLMO学习更多通用表征。</p><h3 id="微调">微调</h3><ul><li>视觉语言检索，用作双编码器</li><li>视觉语言分类，用作融合编码器</li></ul><p><img src="https://s2.loli.net/2023/07/13/eVJjkRx8z5MdlAC.png" /></p>]]></content>
    
    
    
    <tags>
      
      <tag>多模态</tag>
      
      <tag>自监督学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
